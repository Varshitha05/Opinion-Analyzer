{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LEw1PckkA8T5JHHKEqenH9Wi9_AfzF4M","timestamp":1715194299141}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVCJt1UdGkqR","outputId":"c65b77f9-fb62-4734-b3a4-a62c0f179d94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}],"source":["pip install pandas"]},{"cell_type":"code","source":["pip install langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBSexWu7NQnu","outputId":"37affe36-18ec-4508-b3a0-40861dfc7c7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=07247ec18937a3c5db819bcfe6c791b6d76b1f8f040e6e9f12d047d93ac3d66c\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}]},{"cell_type":"code","source":["pip install pyspellchecker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATCuLhp_XGWt","outputId":"0a4b552f-282f-4fbb-dba2-ae159360a00c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspellchecker\n","  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.8.1\n"]}]},{"cell_type":"code","source":["import re\n","import string\n","import nltk\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from langdetect import detect\n","from spellchecker import SpellChecker"],"metadata":{"id":"RiBTlkAxGyy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download stopwords and wordnet\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54kbB2p6Gy42","outputId":"72154d9e-5496-4114-bd59-536b1b93f000"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["def preprocess_text(text):\n","    # Check if the comment is in English\n","    try:\n","        if detect(text) != 'en':\n","            return None\n","    except:\n","        return None\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+', '', text)\n","\n","    text = re.sub(r'[?!]', '', text)\n","\n","    # Remove emojis\n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","                               u\"\\U0001F680-\\U0001F6FF\"\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               \"]+\", flags=re.UNICODE)\n","    text = re.sub(emoji_pattern, '', text)\n","\n","    # Remove special characters and numbers\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    return text\n"],"metadata":{"id":"NEW8Qx-FGy9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read CSV file\n","file_path = '/content/controversy.csv'\n","df = pd.read_csv(file_path)"],"metadata":{"id":"vvTsghU4GzBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess text data in CSV file\n","df['Cleaned_Comments'] = df['Comment'].apply(preprocess_text)\n","\n","# Remove rows where comment is empty\n","df = df.dropna(subset=['Cleaned_Comments'])\n","\n","# Save cleaned data to new CSV file\n","cleaned_file_path = 'controversy_cleaned_comments.csv'\n","df[['Cleaned_Comments']].to_csv(cleaned_file_path, index=False)\n","\n","print(f\"Cleaned data saved to {cleaned_file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOOpwYBzGzFq","outputId":"6705b357-d952-4ced-acf2-b7605e515d6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned data saved to controversy_cleaned_comments.csv\n"]}]}]}